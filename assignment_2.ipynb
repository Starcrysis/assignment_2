{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The second part of the assignment, IDS 2021-2022\n",
    "In this Jupyter notebook document all your results and the way you have obtained them. Please use the _Python environment_ provided for this part of the assignment. In addition to the _Jupyter notebook_, please submit _one zip-file_ containing  other outputs you have generated that are not included in this notebook (such as pdf, jpg, and others). Please make sure that the other outputs are easily identifiable, i.e. use names as requested in the corresponding question. _You do not need to include the datasets._\n",
    "\n",
    "This is the _only_ submission that is required (Jupyter notebook + zip-file). A separate report is _not_ needed and will not be considered for grading. \n",
    "\n",
    "Give your commented Python code and answers in the corresponding provided cells. Make sure to answer all questions in a clear and explicit manner and discuss your outputs. _Please do not change the general structure of this notebook_. You can, however, add additional markdown or code cells if necessary. <b>Please DO NOT CLEAR THE OUTPUT of the notebook you are submitting! </b>\n",
    "\n",
    "<font color=\"red\"> Please make sure to include the names and matriculation numbers of all group members in the slot provided below. </font> If a name or a student id is missing, the student will not receive any points.\n",
    "\n",
    "<font color=\"red\">Plan your time wisely. </font> A few parts of this assignment might take some time to run. It might be necessary to consider time management when you plan your group work.\n",
    "\n",
    "Hint: RWTHmoodle allows multiple submissions, with every new submission overwriting the previous one. <b>Partial submissions are therefore possible and encouraged. </b> This might be helpful in case of technical issues with RWTHMoodle, which may occur close to the deadline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>Student Names and IDs:\n",
    "    \n",
    "    1. Timothy Borrell 436940\n",
    "    \n",
    "    2. \n",
    "    \n",
    "    3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Display\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "### Widgets\n",
    "#import ipywidgets as widgets\n",
    "\n",
    "### Data Handline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### Utility\n",
    "import math\n",
    "import string\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "### Plotting\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "# Matplotlib toolkits\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# Seaborn\n",
    "import seaborn as sns\n",
    "# Plotly\n",
    "import plotly.graph_objects as go\n",
    "# Gespatial data with cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "### Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "### Frequent Pattern Mining\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules as arule\n",
    "\n",
    "### Text Mining\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "### PM4Py\n",
    "import pm4py\n",
    "# Log Handling\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "# Statistics\n",
    "from pm4py.statistics.traces.generic.log import case_statistics\n",
    "# Filtering\n",
    "from pm4py.algo.filtering.log.variants import variants_filter\n",
    "from pm4py.algo.filtering.log.attributes import attributes_filter\n",
    "# Discovery and Conformance Checking\n",
    "from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator\n",
    "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
    "# Visualization\n",
    "from pm4py.visualization.process_tree import visualizer as pt_visualizer\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas indexing\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful if you use the **matplotlib widget** magic. If you do not close the created plots, previous plots may change if you create a new one.\n",
    "So if you use this magic command, be careful about your outputs in your final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Q1 - Preprocessing (20 points)\n",
    "In this question, we consider a US-census dataset (**census_data.csv**).\n",
    "Each row contains statistics of a certain tract on a variety of, particularly income- and work-related, life aspects of US citizens.\n",
    "Short column description:\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ----------- |\n",
    "| CensusTract | Tract |\n",
    "| State | State |\n",
    "| County | County |\n",
    "| TotalPop | Total population  |\n",
    "| Men | Number of men |\n",
    "| Women | Number of women |\n",
    "| Hispanic, White, Black, Native, Asian, Pacific | Percentage of ethnic group |\n",
    "| Citizen | Percentage of citizen |\n",
    "| Income | Median household income |\n",
    "| IncomePerCap | Income per capita |\n",
    "| Poverty | Poverty rate |\n",
    "| ChildPoverty | Child poverty rate |\n",
    "| Professional | Employed in management, business, science, and arts (percentage) |\n",
    "| Service, Office, Construction, Production | Other profession fields (percentage) |\n",
    "| PrivateWork, PublicWork | Employed in private / public sector (percentage) |\n",
    "| Drive, Carpool, Transit, Walk, OtherTransp | Means of commuting (percentage) |\n",
    "| WorkAtHome | Working at home (percentage) |\n",
    "| MeanCommute | Mean time for commuting |\n",
    "| Employed | Number of employed |\n",
    "| SelfEmployed | Self-employed (percentage) |\n",
    "| FamilyWork | Unpaid family work (percentage) |\n",
    "| Unemployment | Unemployment rate |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data and Initial Quality Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Load the dataset into a dataframe `df`. <font color='red'>Use the CensusTract as index for your dataframe</font>. In doing so, ensure that the index is valid, that is, it does not contain any duplicate entries.\n",
    "\n",
    "**In the subsequent questions, only modify the dataframe `df` if explicitly requested. However, you can always create working copies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Show the data types of the dataframe columns as well as the first few rows. On the first sight, are there any data type problems (e.g., numerical columns having a non-numerical data type)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** To improve performance and memory usage (in particular for large datasets) it is important to use **categorical** columns whenever suitable. \n",
    "Are there any categorical column candidates? Explain your answer. \\\n",
    "Afterwards, convert these columns into categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** To select a good strategy to deal with missing data, it is important to get an overview over the general data distribution.\n",
    "Show the basic statistics for the dataset and create 6 boxplots for the following column groupings:\n",
    "\n",
    "    ['TotalPop', 'Men', 'Women', 'Citizen', 'Employed'],\n",
    "    ['Income', 'IncomePerCap'],\n",
    "    ['Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific'],\n",
    "    ['PrivateWork', 'PublicWork', 'SelfEmployed', 'FamilyWork', 'Unemployment', 'Poverty', 'ChildPoverty'],\n",
    "    ['Professional', 'Service', 'Office', 'Construction', 'Production', 'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp', 'WorkAtHome'],\n",
    "    ['MeanCommute']\n",
    "\n",
    "Can you spot any (severe) data quality problems, in particular, are there unrealistic values (also considering the semantics)?\n",
    "\n",
    "*Hint: Use the `df.describe()` function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (boxplots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "(In the following task you can assume that every NAN entry in the dataframe is actually a missing value. This can paritally be justified by the fact that pandas did not have problems inferring the \"proper\" datatypes (e.g., numbers types as string would result in an object column) and your subsequent check of the data types. Therefore, you can use `df.isna()` as a proxy indicator for missing values.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Simply filling missing entries is usually not a good idea. Therefore, you should first analyze the quantity of missing values and check for patterns of missing values.\n",
    "\n",
    "To this end, compute the following statistics on missing values:\n",
    "1) How many entries does the dataframe have? (To relate this to the number of entries missing)\n",
    "2) How many missing values do we have?\n",
    "3) How many rows have at least a single missing value?\n",
    "4) Count the number of missing values per column.\n",
    "5) Count the number of missing values per row and aggregate them - i.e., show the number of rows that suffer from x missing values.\n",
    "\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f)** We decide to **remove all rows from `df` where the total population is zero**. \\\n",
    "Given the preceding results, how do you evaluate this strategy? Try to motivate your argumentation by additional short analysis results (see hint for an inspiration).\n",
    "\n",
    "*Hint: It might be interesting to have a look at the rows with zero population. Afterwards, you can provide some analysis results that show that your (potential) observation generalizes to all rows with zero population.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g)** The previous analysis showed that there are missing values in the 'Men' and 'Women' columns.\\\n",
    "How would you impute these values? \\\n",
    "Motivate your approach and apply it to `df`.\n",
    "\n",
    "*Hint: Do not forget about the semantics of the columns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df[['Men', 'Women']].isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h)** Finally, impute the remaining missing values in `df` using the knn-imputation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Before you impute the remaining missing values, you should improve the data semantics consistency by turning the columns \n",
    "    \n",
    "        ['Men', 'Women', 'Citizen', 'Employed']\n",
    "    \n",
    "    into percentage scores as well. To this end, divide these values by the total population (i.e., 'TotalPop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Impute the missing values using the knn-imputation method.\n",
    "    To this end, apply the following steps:\n",
    "        1) Create a working copy `df_tmp` of your dataframe.\n",
    "        2) Drop the columns `['State', 'County']` from `df_tmp`. On the one hand, this makes the following steps easier because we only have to deal with numerical columns; on the other hand, an alternative one-hot encoding is also problematic as this will cause our feature dimensionality to explode!\n",
    "        3) Normalize the data in `df_tmp` (e.g., Standard score normalization). If the features have very different scales, even though we are mostly using percentages, knn can become very biased.\n",
    "        4) Impute the missing values considering five neighbors.\n",
    "        5) Invert the transformation applied upfront to enable more meaningful and intuitive visualizations.\n",
    "        6) Append the columns `['State', 'County']`\n",
    " \n",
    "In the end, `df` should not contain missing values and have columns `['State', 'County']`.\n",
    "\n",
    "*Hint: Be careful with the indices of your dataframes.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code 1) 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'State' in df.columns\n",
    "assert 'County' in df.columns\n",
    "assert df.isna().sum().sum() == 0\n",
    "assert df['Hispanic'].min() > -0.01\n",
    "assert df['Hispanic'].max() < 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i)** In the final preprocessing step, you should integrate one additional source of data into the preprocessed dataframe `df`. \n",
    "As the data has a natural geospatial dimension, you are going to endow each tract with its geographic coordinate.\n",
    "To this end, load **coordinates.csv**. Integrate the two data sources exploiting the correspondence between 'CensusTract' and 'GEOID'.\n",
    "Finally, drop the 'USPS' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'Men' in df.columns\n",
    "assert 'County' in df.columns\n",
    "assert 'INTPTLONG' in df.columns\n",
    "assert 'INTPTLAT' in df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 - Visualization (15 points)\n",
    "In this task, you will analyze the data that you preprocessed in question 1 (**census_data.csv**). In particular you will analyze income-related aspects, using different means of visualization.\n",
    "\n",
    "Start with the following preprocessed and integrated dataframe `df`. \\\n",
    "Note that it has a similar structure to the dataframe that you should obtain from the previous task, however, the values have been modified!\n",
    "\n",
    "**Library usage:** This notebook imports a couple visualization libraries that have a significat overlap in terms of functionalities. Therefore, you are free to use any of these libraries (and those in the environment in general) to implement the following questions as long as your resulting plot compilies with the explicitly mentioned requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./dataset/df_vis.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Visualize two histograms for 'Income' and 'IncomePerCap' in a **single plot**. Compare the two distributions; what do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation for Visualization Pruposes\n",
    "**b)** As the data contains too many rows for per-row visualizations, you should aggregate the data further before creating more interesting visualizations.\n",
    "The following function will do the job for your; however, why didn't we simply run `groupby(...).mean()` to get the results for the columns\n",
    "specified in `l_col`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_aggregation(df):\n",
    "    l_col = ['Men', 'Women', 'Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific', 'Citizen', 'Income', 'IncomePerCap', \n",
    "             'Poverty', 'ChildPoverty', 'Professional', 'Service', 'Office', 'Construction', 'Production', 'Drive', 'Carpool', \n",
    "             'Transit', 'Walk', 'OtherTransp', 'WorkAtHome', 'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork', 'SelfEmployed', \n",
    "             'FamilyWork', 'Unemployment', 'INTPTLAT', 'INTPTLONG']\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp.loc[idx[:, l_col]] = df_tmp.loc[idx[:, l_col]].mul(df_tmp['TotalPop'], axis=0)\n",
    "    df_tmp = df_tmp.groupby(['State', 'County'], observed=True).sum()\n",
    "    df_tmp.loc[idx[:, l_col]] = df_tmp.loc[idx[:, l_col]].div(df_tmp['TotalPop'], axis=0)\n",
    "    \n",
    "    return df_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = my_aggregation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Next, you should create an overview over column correlations particularly consideirng high/medium/and low incomes.\n",
    "\n",
    "1) Create a copy `df_plot` of the aggregated dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Append a column 'IncomeClass' to `df_plot` containing the 'Income' categories based on the following inter-percentile ranges:\n",
    "    - low' iff the income is less than the 33% income percentile\n",
    "    - 'medium' if the income is between the 33% and 66% percentile, and \n",
    "    - 'high' iff the income is above the 66% percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Project the dataframe on the columns that contain percent values (for the sake of readability), that is:\n",
    "\n",
    "        ['Men', 'Women', 'Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific', 'Poverty', 'ChildPoverty', \n",
    "        'Professional', 'Service', 'Office', 'Construction', 'Production',\n",
    "        'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp', 'WorkAtHome',\n",
    "        'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork', 'SelfEmployed',\n",
    "        'FamilyWork', 'Unemployment', 'IncomeClass']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Create a parallel coordinates diagram that uses the 'IncomeClass' for coloring the lines. Rotate the x-axis labels by 90Â° to make them easier to read.\n",
    "Briefly discuss your results. Do you observe any correlations? Please explain.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Visualization: Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** In this task, you are going to create an advanced visualization that exploits the geospatial nature of the data, that is, you will project the average 'Income' of each county and its population onto a map of the USA. \\\n",
    "You can use the following code to create a suitable map extend.\n",
    "\n",
    "    ax.set_extent([-125, -66.5, 20, 50], ...)\n",
    "        \n",
    "Given this instance, plot one marker (e.g., circular marker) for each row in our aggregated dataset onto the map.\n",
    "The color encoding should show the average 'Income' of the corresponding county, while the size should be chosen according to its population ('TotalPop'). If you want to modify the dataframe, create a **working copy** beforehand.\n",
    "\n",
    "What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 - Frequent Itemsets and Association Rules (12 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Carry out some preprocessing steps before starting the analysis:\n",
    "\n",
    "1) Load the `customer_data.csv`.\n",
    "\n",
    "2) Select 90% of the `customer_data` dataset by random sampling. Use the matriculation number of one of the group members as seed.\n",
    "\n",
    "3) After completing this preprocessing step, export your final dataset as `customer_data_2.csv` and use it for the next steps of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** In this part, we want to get to know our customers by looking at the typical shared characteristics (e.g. \"Married customers in their 40s like wine\"). This would correspond to the itemset {Married, 40s, Wine}. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Create a new dataframe called `customer_data_onehot` such that rows correspond to customers (as in the original data set) and columns correspond to the categories of each of the ten categorical attributes in the data. The new dataframe should only contain boolean values (True/False or 0/1s) such that the value in row $i$ and column $j$ is True (or 1) if and only if the attribute value corresponding to the column $j$ holds for the customer corresponding to row $i$. Display the dataframe.\n",
    "\n",
    "*Hint: For example, for the attribute \"Education\" there are 5 possible categories: 'Graduation', 'PhD', 'Master', 'Basic', '2n Cycle'. Therefore, the new dataframe must contain one column for each of those attribute values.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Use the apriori algorithm to find the frequent itemsets with **min_support = 0.3** from the `customer_data_onehot` dataframe. Show the frequent itemsets that contain at least **3** items.\n",
    "\n",
    "*Hint: The apriori algorithm of mlxtend needs a dataframe containing only boolean values as input.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** In the following we will investigate the effect of using the apriori property when determining the candidates for the frequent itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Implement the following join- and prune steps of the Apriori algorithm: \\\n",
    "   **join function:** a function that, given the frequent itemsets of size k, generates and yields a list of itemsets of size k+1. Only itemsets that share exactly k elements should be merged. \\\n",
    "   **prune function:** Given the set of candidate itemsets of size k+1 and the set of frequent itemsets of size k, remove the candidate sets that contain an infrequent subset of size k and return the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (join function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (prune function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) To see the effect of the apriori property, compare the number of candidate itemsets of size 4 obtained with and without pruning from the itemsets of size three for different values for min_support. To this end, generate a list of tuples *(min_sup, C4_size, C4_size_pruned, L4_size)* as follows:\n",
    "\n",
    "For $\\textrm{min_support} \\in [0.1,0.2,...,0.8,0.9,1]$, repeat:\n",
    "\n",
    "1. Obtain all frequent itemsets of size three using the apriori algorithm.\n",
    "\n",
    "2. Using the result from 1., generate all itemsets of size four by applying your **join function** $\\rightarrow$ C4_size.\n",
    "\n",
    "3. Prune the result from 2. using your **prune function** $\\rightarrow$ C4_size_pruned.\n",
    "\n",
    "4. Compute the frequent itemsets of size four by using the apriori algorithm $\\rightarrow$ L4_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Plot the number of candidate sets with and without pruning and the number of frequent itemsets of size four against the corresponding min_sup value. Interpret the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Use the FP-Growth algorithm to obtain all frequent itemsets with **min_support = 0.3** from `customer_data_onehot`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** In the following, you should generate association rules from the frequent itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "1) Using only the frequent itemsets with min_support=0.3, generate different association rules using minimum confidence equal to 0.6 as a metric. Show the association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) From the association rules obtained in task (d) 1), provide the three rules with the highest lift. Comment on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 - Text Mining (15 points)\n",
    "In this question, you will use the scripts of some Harry Potter movies. First, you will try to predict the character given a line in the script. Afterwards, using N-grams, you will generate sentences for some of the characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** In this part, you will preprocess and reconstruct the data to make it suitable for the following tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Load each of the datasets <b>hp_1.csv</b>, <b>hp_2.csv</b>, and <b>hp_3.csv</b> into its own dataframe and show the set of characters (here: a fictional character) appearing in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Merge the three datasets into a single dataframe called `hp_df` that comprises only the lines spoken from one of the four characters *Harry, Hermione, Dumbledore, and Snape*. Your new dataframe must contain two columns: one for the (four) characters and the other for the lines. You can name those columns \"Character\" and \"Sentence\" as in the original data. \\\n",
    "    Make sure that `hp_df` contains a single unique spelling for each of the characters. \\\n",
    "    Make sure that `hp_df` includes all lines (here: script lines) of a character even if this character is spelled slightly differently (e.g., Dumbledore or dumbledore) in the original dataset. \\\n",
    "    Show the first few lines of your dataframe.\n",
    "    \n",
    "*Hint: Be aware of white space characters!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Create the `hp_sampled` dataset which includes 90% of the `hp_df` data. Use the matriculation number of one of the group members as seed. Export the sampled dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** In this part, you are going to train a classifier that, given a line from the script, predicts the character. For each character, the data contains many sentences belonging to that character. Note that sometimes the \"Sentence\" column in the original dataset contains more than one sentence. The set of sentences for each character should be seen as the set of example documents belonging to that character (the class). Each individual sentence is a single document. The whole corpus consists of all the individual sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Create a new dataframe called `hp_processed` from the dataframe `hp_sampled` such that the new dataframe contains again the columns \"Character\" and \"Sentence\", but every entry in the \"Sentence\" column must be a single sentence. Display the shape of the dataframe and compare it to the shape of `hp_sampled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Split the preprocessed data `hp_processed` into training (80%) and test (20%) data preserving the distribution based on \"Character\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Preprocess the training and test corpus (to lowercase, no punctuation, tokenization, lemmatization, and stopword removal) and obtain a boolean document-term matrix (i.e, each row in the matrix contains only 1s and 0s depending on whether a particular word appears in a sentence or not). Train a logistic classifier on the training corpus with the character as target feature. Use the classifier to predict the character of the sentences in the test corpus and show its accuracy on the test corpus. Comment on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Next, you are going to perform the same predicting task based on doc2vec.\n",
    "\n",
    "1. Preprocess the training corpus (to lowercase, no punctuation, tokenization, lemmatization, and stopword removal). \n",
    "2. Create a doc2vec model to reduce the dimension of the document vector. Choose a vector size 4-8 and ignore all words whose count is lower than 3.\n",
    "3. Train the doc2vec model on the training data (thus creating an embedding).\n",
    "4. Use the created embedding to convert the training set to a set of document vectors.\n",
    "5. Train a logistic classifier on the train data with the character as target feature.\n",
    "6. Show the accuracy of prediction on the test data and comment on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** For the following tasks use the `hp_processed` (the data before splitting into training and test data).\n",
    "\n",
    "1) For each character, create a list containing all sentences of that character.\n",
    "    For each character separately, build a bigram language model using MLE. Do not perform stemming and stopword removal for this task, but apply other preprocessing steps such as to lowercase, no punctuation, and tokenization. Use both right and left padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) For each character, use the created language model to generate a sentence of ten words. Display the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Build a 4-gram model with the same data as in the previous task. Use both right and left padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) For each character, use the created 4-gram language model to generate a sentence of ten words. Display the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Compare the sentences generated by the bigram model with the sentences generated by the 4-gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Q5 - Process Mining (23 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we consider a simulated process of students that participate in an online course.\n",
    "The course comprises 6 batches of lecture material as well as a mandatory assignment to be delivered in two parts. (Note that in this process, it is not required to achieve a certain score in the assignment in order to be admitted to the exam.)\n",
    "\n",
    "While there are strict deadlines for the assignment and the exam, there is only a recommended schedule for the lecture material (i.e., consume material in order).\n",
    "\n",
    "The system logs for every student, among other activities, when he downloads a certain lecture material batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data and Basic Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Load the data **log.csv** and create a PM4Py event log. In doing so, use the following column mapping:\n",
    " - 'Activity' is the activity key\n",
    " - 'Student' is the case ID\n",
    " - 'Timestamp' is the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Compute the following basic information:\n",
    "- Number of events\n",
    "- Number of cases\n",
    "- Earliest timestamp\n",
    "- Latest timestamp\n",
    "- Number of trace variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Usually, it is insightful to have a look at the distribution of the variants in terms of how often a certain variant is present in the log.\n",
    "Therefore, create a **scatter plot** that shows the distribution of the variants as follows:\n",
    "- x-axis: The variants (in ascending order of their support)\n",
    "- y-axis: Frequency of the variant in the log (total or relative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** While the variant distribution shows potential standard process executions in terms of the activity ordering, the distribution of the case durations shows the typical timeframe of cases.\n",
    "Create a histogram plot over the case durations. For the sake of readability, make sure that the x-axis labels (in this case the case durations) have an easily readable format, that is, your x-axis labels should look like this:\n",
    "<br></br>\n",
    "<div>\n",
    "<img src=\"templates/caseDurationXAxis.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovery and Conformance Checking\n",
    "Next, you are going to discover process models for different perspectives on the process. Moreover, you will evaluate how well the process models can represent the behavior present in the log (i.e., the fitness of the models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Before discovering models, create three addtional perspectives onto the process by creating three additional event log from the log loaded in a):\n",
    "\n",
    "1. Log containing only 30% of the most frequent traces (**log_varaint03**)\n",
    "2. Log containing only 50% of the most frequent traces (**log_varaint05**)\n",
    "3. Log containing only students that take the exam, that is, cases that end with 'Exam' (**log_exam**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Inductive Miner and Replay Fitness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**f)** To get a better understanding of the processes in our four event logs (base log + three additional logs), create processes models using the **Inductive Miner**.\n",
    "Concretely, for each of the 4 event logs, create two process models using the Inductive Miner with noise threshold **0 and 0.2**. Moreover, to access how well the model presents the logged behavior, apply conformance checking in terms of token-based replay to the model and the log from which it has been mined. Visualize each model as a process tree and as the corresponding Petri net.\n",
    "\n",
    "In total, your cells should output 8 conformance scores, 8 process trees, and 8 Petri nets (for each log + noise threshold combination). Make sure that it is clear which model and conformance score belongs to log and parameter configuration.\n",
    "\n",
    "For example, your output can look like this\n",
    "#### Log: Base\n",
    "##### IM threshold 0\n",
    ">Fitness score\n",
    "\n",
    ">Picture of the process tree\n",
    "\n",
    ">Picture of the Petri net\n",
    "\n",
    "##### IM threshold 0.2\n",
    ">Fitness score\n",
    "\n",
    ">Picture of the process tree\n",
    "\n",
    ">Picture of the Petri net\n",
    "\n",
    "**Describe your results**. How well do the models fit and, in particular, how do the models for\n",
    "- log_variant05 and log (base log)\n",
    "- log_variant05 and log_variant03\n",
    "- log base and log_exam\n",
    "\n",
    "differ in terms of the behavior that they allow?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths of Excellence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g)** As lectures are in a constant urge to improve their courses in a way that participants learn as much as possible ;), you are facing the research question to identify *paths of excellence*.\n",
    "In particular, you should identify how the studying behavior differs between excellent students (**final exam score greater than or equal to 85**) and non-excellent students (**final exam score less than 85**).\n",
    "Try to answer this research questions using techniques from the preceding Process Mining questions. \n",
    "\n",
    "*Hint: There is no single unique solution (e.g., in terms of parameter choice); therefore, it suffices if your \"design choices\" are reasonble.* \\\n",
    "*Hint: The final exam score is point score that is associated with the \"Exam\" event.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance and Frequency Decoration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h)** While the discovery of a process model is the most prototypical step in a process mining analysis, its enrichment by frequency and performance statistics is a very common step too. To this end, enrich the Petri net that you discovered for log_exam using Inductive Miner with noise threshold 0.2 by frequency and performance information. Plot two Petri nets decorated with frequency and performance information, respectively.\n",
    "\n",
    "Describe your results. Can you observe any problems (in particular with respect to the initial process description)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Mining Meets Advanced Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying Activity Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** In this task, we are going to use advanced visualization techniques to create an overview over the course acitivities over time (**log.csv**).\n",
    "In particular, you shall create a heatmap that shows how often activities occur in a particular week. \n",
    "Your heatmap should adhere to the following specification:\n",
    "- y-axis: Shows the activity labels\n",
    "- x-axis: Time in terms of course weeks. See the following example snippet:\n",
    "<div>\n",
    "<img src=\"templates/PMAV_HeatmapXAxisSnippet.png\" width=\"100\"/>\n",
    "</div>\n",
    "- data: The bucket counts should be derived from **log.csv**\n",
    "\n",
    "Using this configuration, the bucket 2021-11-24 till 2021-12-01 with y-axis label \"Exam\" and value v would be read as:\n",
    "In the week between 2021-11-24 and 2021-12-01 v exams took place.\n",
    "\n",
    "Describe your result. Which patterns do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Can you relate the patterns that you observe in the heatmap to the process models that you discovered in question **Q5 - f)**?\n",
    "\n",
    "*Hint: In contrast to the other questions, this question is deliberately less explicit. You may approach it having the following question in mind: \\\n",
    "Is there a pattern in the heatmap that explains why a certain process model shows a certain behavior/control flow?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Assume that another process analyst also had access to the event log. \n",
    "Given the data, he created the following novel entities:\n",
    "- 'Block 1 Complete': The participant downloaded the entire material of the first lecture block (materials 1, 2, and 3). (Not considering when he downloaded it)\n",
    "- 'Block 1 Incomplete': The participant did not download the entire material of the first lecture block\n",
    "- 'Block 2 Complete': See 'Block 1 Complete'\n",
    "- 'Block 2 Incomplete': See 'Block 1 Incomplete'\n",
    "- 'Ass 1 Excellent': Participant scored at least 85 points in the first part of the assignment\n",
    "- 'Ass 1 Not Excellent': Participant scored less than 85 points in the part of the assignment\n",
    "- 'Ass 2 Not Excellent', 'Ass 2 Excellent', 'Exam Not Excellent', 'Exam Excellent': Similar to 'Ass 1 Excellent' and 'Ass 1 Not Excellent'\n",
    "- 'Withdraw': Participant dropped the course\n",
    "\n",
    "Based on the entities he derived a set of flow, for example, the flow between 'Block 1 Complete' and 'Ass 1 Excellent' describes how often a partipant who consumed the first lecture block scored excellent in the first part of the assignment.\n",
    "\n",
    "The next cell loads the entities and flows for you. The flows are stored as a dictionary following the pattern:\n",
    "\n",
    "    (source, target): flow_value\n",
    "    \n",
    "where source and target are indices into the entities list.\n",
    "\n",
    "Create a **Sankey diagram** that visulizes these flows. Please use `plotly.graph_object.Sankey` to create the diagram.\n",
    "\n",
    "Briefly **describe** your results.\n",
    "Moreover, **discuss** this visualization considering your knowlege from the Process Mining task.\n",
    "\n",
    "*Hint: Having a look at the Sankey diagram will make the analyst's idea behind the entities much clearer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/sankeyEntities.pkl', 'rb') as f:\n",
    "    entities = pickle.load(f)\n",
    "with open('./dataset/sankeyFlows.pkl', 'rb') as f:\n",
    "    flows = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your markdown for your discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6 - Big Data (15 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description\n",
    "You are working at a finance company that makes loans to individuals and businesses. As a process analyst in *business intelligence team*, you are expected to deliver data-driven insights to improve business processes of the company. Recently, your boss asked you to discover a comprehensive process model of 10 international branches using your big data skills. Your colleague already tried it using commercial on-premise tools, but, due to the immense size of the data, he didn't manage to even load the data to the tool. You are planning to 1) load the datasets from 10 different branches to Hadoop Distributed File System (HDFS), 2) preprocess them using HDFS, and 3) use MapReduce programming model to discover a comprehensive process model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "The preparation of this problem consists of two steps:\n",
    "\n",
    "**Preparation step 1**: Replace the filepath to your own filepath to produce the **LoanApplication.csv**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#your filepath\n",
    "filepath = \"./dataset/LoanApplication.csv\"\n",
    "original_log = pd.read_csv(filepath,sep=\",\")\n",
    "original_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation step 2**: In this question, we generate 10 event logs based on the ``original_log``. For randomization, you need to use the sum of the group's matriculation numbers (e.g., a group with 3 students having \"100000\", \"100001\", and \"100002\" as their matriculation numbers will use \"300003\" for the randomization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are GIVEN utility functions (do not modify):\n",
    "import random\n",
    "import os\n",
    "def _ramdomize(x):\n",
    "    random_val = random.randint(5,10)\n",
    "    return x+random_val\n",
    "\n",
    "def _randomize_log(log,matriculation_num):\n",
    "    \"\"\"Randomize case attributes based on the matriculation number\n",
    "\n",
    "    Keyword arguments:\n",
    "    log -- event log\n",
    "    matriculation_num - sum of matriculation numbers\n",
    "    \"\"\"\n",
    "    attribute_cols = [\"Duration\"]\n",
    "    random.seed(matriculation_num)\n",
    "    for attr in attribute_cols:\n",
    "        log[attr] = log[attr].apply(_ramdomize)\n",
    "    return log\n",
    "\n",
    "def _extract_log(log,iter_num):\n",
    "    \"\"\"Extract n-th log to ./generated_logs/\n",
    "\n",
    "    Keyword arguments:\n",
    "    log -- event log\n",
    "    iter_num -- n-th iteration\n",
    "    \"\"\"\n",
    "    log.to_csv(\"./generated_logs/generated_log-{}.tsv\".format(iter_num),header=False,index=False, sep=\"\\t\",line_terminator=\"\")\n",
    "\n",
    "def generate_log(original_log,num_replication,mat_num):\n",
    "    \"\"\"Generate logs (randomized by the matriculation number and extracted to ./generated_logs/) \n",
    "\n",
    "    Keyword arguments:\n",
    "    log -- event log\n",
    "    num_replication -- number of generated logs\n",
    "    mat_num -- sum of matriculation numbers\n",
    "    \"\"\"\n",
    "    case_col=\"CaseID\"\n",
    "    timestamp_col = \"Timestamp\"\n",
    "    dir_path = \"./generated_logs\"\n",
    "    try:\n",
    "        os.mkdir(dir_path)\n",
    "    except OSError:\n",
    "        print (\"Directory already exists: %s\" % dir_path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % dir_path)\n",
    "    \n",
    "    for i in range(num_replication):\n",
    "        print(\"starts {}\".format(i))\n",
    "        generated_log = original_log.copy(deep=True)\n",
    "        generated_log[case_col] = str(i) + generated_log[case_col]\n",
    "        generated_log[timestamp_col] = generated_log[timestamp_col].apply(str).str.zfill(10)\n",
    "        randomized_log = _randomize_log(generated_log,mat_num)\n",
    "        _extract_log(randomized_log,i)\n",
    "        print (\"Successfully created %i th log at %s \"% (i,dir_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the SUM_MAT_NUM to yours to generate logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "SUM_MAT_NUM = 154031 \n",
    "NUM_REPITITION=10\n",
    "generate_log(original_log,NUM_REPITITION,SUM_MAT_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Hadoop Distributed File System (HDFS)\n",
    "\n",
    "Now, it's time to work with the Hadoop Distributed File System (HDFS). The goal of this task is to merge 10 event logs at your disk using HDFS. Follow the instructions below and show your results in each step (screenshots of the command line). We use \"letter identifier\" for this task (The letter identifier is the string consisting of the first letters of the group memebers' first names, e.g., for the group with \"Antonio RÃ¼diger\", \"Bernd Leno\", \"Christian GÃ¼nter\", the indentifier is \"ABC\").\n",
    "\n",
    "    1) Import the event logs to your Docker engine (at /usr/local/hadoop/(identifier)-generated-logs/).\n",
    "    2) Upload the event logs to the running HDFS (at /input/(identifier)-generated-logs/). \n",
    "    3) Merge all the files and copy the result back to HDFS (at /input/(identifier)-final-log-10.tsv).\n",
    "    4) Merge 6 files (you can randomly select) and copy the result back to HDFS (at /input/(identifier)-final-log-6.tsv).\n",
    "    5) Merge 2 files (you can randomly select) and copy the result back to HDFS (at /input/(identifier)-final-log-2.tsv).\n",
    "    6) Print out the completely-merged event log from 3), i.e., \"(identifier)-final-log-10.tsv\", in the command line (the screenshot may contain 10 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "from IPython.display import Image\n",
    "# Image(filename='your_path_to_screenshot_of_a1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "#Image(filename='your_path_to_screenshot_of_a2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "#Image(filename='your_path_to_screenshot_of_a3') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "#Image(filename='your_path_to_screenshot_of_a4') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Process Discovery\n",
    "\n",
    "Discover a process model from the completely merged event log using MapReduce algorithms. Explain how you discover the process model with the following deliverables:\n",
    "\n",
    "    1) Mapper function (as python file(s))\n",
    "    2) Reducer function (as python file(s))\n",
    "    3) Hadoop commands for MapReduce computation (as text file)\n",
    "    4) Jupyter notebook script that visualize (1) a directly-follows graph and (2) a Petri net  based on the computed directly-follows relations.\n",
    "\n",
    "<font color='red'>Important!</font> Please note that in this task, your result will be evaluated based on whether they are reproducible from your explanation. If you skip MapReduce computations for this task, you will get 0 points.The deliverables of 1), 2), and 3) should be submitted as outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Performance Analysis\n",
    "\n",
    "a) Compute the total service time for each case based on MapReduce algorithms using the completely-merged event log (i.e., (identifier)-final-log-10.tsv) and visualize 100 cases that show the longest total service time using any chart.\n",
    "    \n",
    "The deliverables of 1), 2), 3) and 4) should be submitted as outputs:\n",
    "```\n",
    "1) Mapper function (as python file(s))\n",
    "2) Reducer function (as python file(s))\n",
    "3) Hadoop commands for MapReduce calculation (as text file)\n",
    "4) Result: total service times for cases (as text file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compare the (approximate) computation time of the service time calculation between 1. the completely-merged event log (i.e., (identifier)-final-log-10.tsv), 2. 6-merged event log (i.e., (identifier)-final-log-6.tsv), and 3. 2-merged event log (i.e., (identifier)-final-log-2.tsv). Interpret the difference (e.g., the computation time scales linearly with the increasing number of events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
